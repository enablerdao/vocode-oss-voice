#!/usr/bin/env python3
"""
é«˜å“è³ªæ—¥æœ¬èªéŸ³å£°ä¼šè©±ãƒ‡ãƒ¢
- Google Speech Recognition: æ—¥æœ¬èªéŸ³å£°èªè­˜
- Edge TTS: é«˜å“è³ªæ—¥æœ¬èªéŸ³å£°åˆæˆï¼ˆMicrosoftï¼‰
- OpenAI GPT-3.5: æ—¥æœ¬èªAIä¼šè©±
"""

import asyncio
import signal
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

sys.path.insert(0, str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent.parent))

from edge_tts_synthesizer import EdgeTTSSynthesizer, EdgeTTSSynthesizerConfig
from high_performance_oss_demo import OptimizedGoogleSRTranscriberConfig, OptimizedGoogleSRTranscriber
from vocode.helpers import create_streaming_microphone_input_and_speaker_output
from vocode.logging import configure_pretty_logging
from vocode.streaming.agent.chat_gpt_agent import ChatGPTAgent
from vocode.streaming.models.agent import ChatGPTAgentConfig
from vocode.streaming.models.message import BaseMessage
from vocode.streaming.streaming_conversation import StreamingConversation

configure_pretty_logging()
load_dotenv()


async def list_voices():
    """åˆ©ç”¨å¯èƒ½ãªéŸ³å£°ã‚’è¡¨ç¤º"""
    await EdgeTTSSynthesizer.get_voices_list()


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        print("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    print("=" * 60)
    print("ğŸŒ é«˜å“è³ªæ—¥æœ¬èªéŸ³å£°ä¼šè©±ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    print()
    print("ğŸ¤ éŸ³å£°èªè­˜: Google Speech Recognition")
    print("ğŸ”Š éŸ³å£°åˆæˆ: Edge TTS (Microsoft Neural Voices)")
    print("ğŸ¤– AI: OpenAI GPT-3.5")
    print()
    print("åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªéŸ³å£°:")
    print("- ja-JP-NanamiNeural (å¥³æ€§ãƒ»æ¨™æº–)")
    print("- ja-JP-AoiNeural (å¥³æ€§ãƒ»æ˜ã‚‹ã„)")
    print("- ja-JP-KeitaNeural (ç”·æ€§ãƒ»æ¨™æº–)")
    print("- ja-JP-DaichiNeural (ç”·æ€§ãƒ»æ˜ã‚‹ã„)")
    print()
    
    # éŸ³å£°ã®é¸æŠ
    voice_choice = input("éŸ³å£°ã‚’é¸æŠ (1-4, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1): ").strip() or "1"
    voices = {
        "1": "ja-JP-NanamiNeural",
        "2": "ja-JP-AoiNeural", 
        "3": "ja-JP-KeitaNeural",
        "4": "ja-JP-DaichiNeural"
    }
    selected_voice = voices.get(voice_choice, "ja-JP-NanamiNeural")
    
    print(f"\né¸æŠã•ã‚ŒãŸéŸ³å£°: {selected_voice}")
    print("-" * 60)
    
    # ãƒã‚¤ã‚¯ã¨ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã®è¨­å®š
    (
        microphone_input,
        speaker_output,
    ) = create_streaming_microphone_input_and_speaker_output(
        use_default_devices=True,
    )
    
    # æ—¥æœ¬èªAIè¨­å®š
    agent = ChatGPTAgent(
        ChatGPTAgentConfig(
            openai_api_key=openai_api_key,
            initial_message=BaseMessage(text="ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯é«˜å“è³ªãªéŸ³å£°ã§ãŠè©±ã—ã§ãã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä½•ã‹ãŠèãã«ãªã‚ŠãŸã„ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"),
            prompt_preamble="""ã‚ãªãŸã¯è¦ªåˆ‡ã§æ˜ã‚‹ã„æ—¥æœ¬èªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨æ¥½ã—ãä¼šè©±ã—ã¦ãã ã•ã„ã€‚
            å›ç­”ã¯è‡ªç„¶ã§ä¼šè©±çš„ãªãƒˆãƒ¼ãƒ³ã‚’ä¿ã¡ã€é©åº¦ã«æ„Ÿæƒ…ã‚’è¾¼ã‚ã¦ãã ã•ã„ã€‚
            ç›¸æ‰‹ã®è©±ã‚’ã‚ˆãèãã€å…±æ„Ÿçš„ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚
            æ™‚ã«ã¯ã€Œãã†ã§ã™ã­ã€ã€Œãªã‚‹ã»ã©ã€ãªã©ã®ç›¸æ§Œã‚‚ä½¿ã£ã¦ãã ã•ã„ã€‚""",
            model_name="gpt-3.5-turbo",
            generate_responses=True,
        )
    )
    
    # ä¼šè©±ã®è¨­å®š
    conversation = StreamingConversation(
        output_device=speaker_output,
        transcriber=OptimizedGoogleSRTranscriber(
            OptimizedGoogleSRTranscriberConfig.from_input_device(
                microphone_input,
                language="ja-JP",
                energy_threshold=300,
                pause_threshold=0.8,
                chunk_duration_seconds=0.5,
            ),
        ),
        agent=agent,
        synthesizer=EdgeTTSSynthesizer(
            EdgeTTSSynthesizerConfig.from_output_device(
                speaker_output,
                voice=selected_voice,
                rate="+5%",  # å°‘ã—é€Ÿã‚ã§è‡ªç„¶ã«
                volume="+0%",
                pitch="+0Hz",
            ),
        ),
    )
    
    await conversation.start()
    print("\nğŸ™ï¸  é«˜å“è³ªéŸ³å£°ä¼šè©±ã‚’é–‹å§‹ã—ã¾ã—ãŸï¼")
    print("ğŸ’¡ æ—¥æœ¬èªã§è‡ªç„¶ã«è©±ã—ã‹ã‘ã¦ãã ã•ã„")
    print("ğŸ›‘ çµ‚äº†: Ctrl+C")
    print("-" * 60)
    
    # Ctrl+Cã§çµ‚äº†
    signal.signal(signal.SIGINT, lambda _0, _1: asyncio.create_task(conversation.terminate()))
    
    # éŸ³å£°å…¥åŠ›ã‚’å‡¦ç†
    while conversation.is_active():
        chunk = await microphone_input.get_audio()
        conversation.receive_audio(chunk)


if __name__ == "__main__":
    print("\nğŸŒ é«˜å“è³ªæ—¥æœ¬èªéŸ³å£°ä¼šè©±ã‚·ã‚¹ãƒ†ãƒ ï¼ˆEdge TTSç‰ˆï¼‰\n")
    
    # éŸ³å£°ãƒªã‚¹ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹
    if len(sys.argv) > 1 and sys.argv[1] == "--list-voices":
        asyncio.run(list_voices())
    else:
        try:
            asyncio.run(main())
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ã”åˆ©ç”¨ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼")
        except EOFError:
            # éå¯¾è©±ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å®Ÿè¡Œ
            print("éå¯¾è©±ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°ä½¿ç”¨ï¼‰")
            # asyncio.run(main_no_input())