#!/usr/bin/env python3
"""
é«˜æ€§èƒ½OSSéŸ³å£°ä¼šè©±ãƒ‡ãƒ¢
- Google Speech Recognition (æœ€é©åŒ–ç‰ˆ) - ç„¡æ–™éŸ³å£°èªè­˜
- gTTS - ç„¡æ–™éŸ³å£°åˆæˆ
- OpenAI GPT-3.5 - AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
"""

import asyncio
import signal
import os
import sys
import io
import wave
import queue
import threading
import time
import numpy as np
from pathlib import Path
from dotenv import load_dotenv
import speech_recognition as sr
from concurrent.futures import ThreadPoolExecutor

sys.path.insert(0, str(Path(__file__).parent))

from vocode.helpers import create_streaming_microphone_input_and_speaker_output
from vocode.logging import configure_pretty_logging
from vocode.streaming.agent.chat_gpt_agent import ChatGPTAgent
from vocode.streaming.agent.echo_agent import EchoAgent
from vocode.streaming.models.agent import ChatGPTAgentConfig, EchoAgentConfig
from vocode.streaming.models.message import BaseMessage
from vocode.streaming.models.synthesizer import GTTSSynthesizerConfig
from vocode.streaming.models.transcriber import TranscriberConfig, Transcription
from vocode.streaming.streaming_conversation import StreamingConversation
from vocode.streaming.synthesizer.gtts_synthesizer import GTTSSynthesizer
from vocode.streaming.transcriber.base_transcriber import BaseThreadAsyncTranscriber
from pydub import AudioSegment

configure_pretty_logging()
load_dotenv()


class OptimizedGoogleSRTranscriberConfig(TranscriberConfig):
    language: str = "en-US"
    energy_threshold: int = 300
    pause_threshold: float = 0.5
    phrase_threshold: float = 0.3
    non_speaking_duration: float = 0.5
    chunk_duration_seconds: float = 0.5


class OptimizedGoogleSRTranscriber(BaseThreadAsyncTranscriber[OptimizedGoogleSRTranscriberConfig]):
    """é«˜æ€§èƒ½ç‰ˆGoogle Speech Recognition Transcriber"""
    
    def __init__(self, transcriber_config: OptimizedGoogleSRTranscriberConfig):
        super().__init__(transcriber_config)
        self.recognizer = sr.Recognizer()
        self.recognizer.energy_threshold = transcriber_config.energy_threshold
        self.recognizer.pause_threshold = transcriber_config.pause_threshold
        self.recognizer.phrase_threshold = transcriber_config.phrase_threshold
        self.recognizer.non_speaking_duration = transcriber_config.non_speaking_duration
        
        # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒãƒƒãƒ•ã‚¡è¨­å®š
        self.chunk_duration = transcriber_config.chunk_duration_seconds
        self.chunk_size = int(transcriber_config.sampling_rate * self.chunk_duration)
        self.audio_queue = queue.Queue()
        self.is_processing = False
        self._ended = False
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«
        self.executor = ThreadPoolExecutor(max_workers=2)
        
        # éŸ³å£°ãƒãƒƒãƒ•ã‚¡
        self.current_buffer = []
        self.silence_counter = 0
        self.max_silence_chunks = int(1.0 / self.chunk_duration)  # 1ç§’ã®ç„¡éŸ³
        
    def _run_loop(self):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ«ãƒ¼ãƒ—"""
        # éŸ³å£°å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        processing_thread = threading.Thread(target=self._process_audio_loop)
        processing_thread.daemon = True
        processing_thread.start()
        
        while not self._ended:
            try:
                # éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
                chunk = self.input_janus_queue.sync_q.get()
                
                # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                self.current_buffer.append(chunk)
                
                # ä¸€å®šã‚µã‚¤ã‚ºã«ãªã£ãŸã‚‰å‡¦ç†ã‚­ãƒ¥ãƒ¼ã«é€ã‚‹
                if len(self.current_buffer) * len(chunk) >= self.chunk_size * 2:
                    audio_data = b''.join(self.current_buffer)
                    self.audio_queue.put(audio_data)
                    self.current_buffer = []
                    
            except Exception as e:
                print(f"Error in audio loop: {e}")
                
    def _process_audio_loop(self):
        """éŸ³å£°å‡¦ç†ãƒ«ãƒ¼ãƒ—ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰"""
        accumulated_audio = io.BytesIO()
        wav_writer = wave.open(accumulated_audio, 'wb')
        wav_writer.setnchannels(1)
        wav_writer.setsampwidth(2)
        wav_writer.setframerate(self.transcriber_config.sampling_rate)
        
        speech_detected = False
        
        while not self._ended:
            try:
                # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                audio_data = self.audio_queue.get(timeout=0.1)
                
                # WAVãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€
                wav_writer.writeframes(audio_data)
                
                # éŸ³å£°ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ¬ãƒ™ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
                audio_segment = AudioSegment(
                    audio_data,
                    frame_rate=self.transcriber_config.sampling_rate,
                    sample_width=2,
                    channels=1
                )
                
                # RMSã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¨ˆç®—
                rms = audio_segment.rms
                
                if rms > self.transcriber_config.energy_threshold:
                    speech_detected = True
                    self.silence_counter = 0
                else:
                    self.silence_counter += 1
                
                # éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã€ãã®å¾Œç„¡éŸ³ãŒç¶šã„ãŸå ´åˆã«èªè­˜ã‚’å®Ÿè¡Œ
                if speech_detected and self.silence_counter >= self.max_silence_chunks:
                    # éŸ³å£°èªè­˜ã‚’å®Ÿè¡Œ
                    wav_writer.close()
                    accumulated_audio.seek(0)
                    
                    try:
                        audio_for_recognition = AudioSegment.from_wav(accumulated_audio)
                        audio_data = sr.AudioData(
                            audio_for_recognition.raw_data,
                            sample_rate=audio_for_recognition.frame_rate,
                            sample_width=audio_for_recognition.sample_width
                        )
                        
                        # Google Speech Recognitionã§èªè­˜
                        text = self.recognizer.recognize_google(
                            audio_data,
                            language=self.transcriber_config.language
                        )
                        
                        if text:
                            print(f"ğŸ¤ èªè­˜: {text}")
                            # èªè­˜çµæœã‚’é€ä¿¡
                            self.produce_nonblocking(
                                Transcription(
                                    message=text,
                                    confidence=0.95,
                                    is_final=True
                                )
                            )
                    
                    except sr.UnknownValueError:
                        pass  # éŸ³å£°ãŒèªè­˜ã§ããªã‹ã£ãŸ
                    except sr.RequestError as e:
                        print(f"Google Speech Recognition error: {e}")
                    
                    # ãƒãƒƒãƒ•ã‚¡ã‚’ãƒªã‚»ãƒƒãƒˆ
                    accumulated_audio = io.BytesIO()
                    wav_writer = wave.open(accumulated_audio, 'wb')
                    wav_writer.setnchannels(1)
                    wav_writer.setsampwidth(2)
                    wav_writer.setframerate(self.transcriber_config.sampling_rate)
                    speech_detected = False
                    self.silence_counter = 0
                    
            except queue.Empty:
                continue
            except Exception as e:
                print(f"Error in processing loop: {e}")
                
        if wav_writer:
            wav_writer.close()
            
    async def terminate(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self._ended = True
        self.executor.shutdown(wait=False)


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    openai_api_key = os.getenv("OPENAI_API_KEY")
    use_openai = bool(openai_api_key)
    
    print("=" * 60)
    print("ğŸš€ é«˜æ€§èƒ½OSSéŸ³å£°ä¼šè©±ãƒ‡ãƒ¢")
    print("=" * 60)
    print()
    print("ğŸ¤ éŸ³å£°èªè­˜: Google Speech Recognition (æœ€é©åŒ–ç‰ˆ)")
    print("ğŸ”Š éŸ³å£°åˆæˆ: gTTS (Google Text-to-Speech)")
    print(f"ğŸ¤– AI: {'OpenAI GPT-3.5' if use_openai else 'Echo Agent'}")
    print()
    print("ç‰¹å¾´:")
    print("- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜")
    print("- ä½é…å»¶ãƒ¬ã‚¹ãƒãƒ³ã‚¹")
    print("- è‡ªç„¶ãªä¼šè©±ãƒ•ãƒ­ãƒ¼")
    print()
    
    # ãƒã‚¤ã‚¯ã¨ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã®è¨­å®š
    (
        microphone_input,
        speaker_output,
    ) = create_streaming_microphone_input_and_speaker_output(
        use_default_devices=True,
    )
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®é¸æŠ
    if use_openai:
        agent = ChatGPTAgent(
            ChatGPTAgentConfig(
                openai_api_key=openai_api_key,
                initial_message=BaseMessage(text="Hi! I'm your AI assistant with optimized speech recognition. Let's have a conversation!"),
                prompt_preamble="""You are a helpful and friendly AI assistant. Keep responses concise and conversational. 
                Respond naturally as if in a spoken conversation.""",
                model_name="gpt-3.5-turbo",
                generate_responses=True,
            )
        )
    else:
        agent = EchoAgent(
            EchoAgentConfig(
                initial_message=BaseMessage(text="Hello! I'm an echo agent with optimized speech recognition!"),
            )
        )
    
    # ä¼šè©±ã®è¨­å®š
    conversation = StreamingConversation(
        output_device=speaker_output,
        transcriber=OptimizedGoogleSRTranscriber(
            OptimizedGoogleSRTranscriberConfig.from_input_device(
                microphone_input,
                language="en-US",
                energy_threshold=300,
                pause_threshold=0.5,
                chunk_duration_seconds=0.5,
            ),
        ),
        agent=agent,
        synthesizer=GTTSSynthesizer(
            GTTSSynthesizerConfig.from_output_device(
                speaker_output,
                lang="en",
                tld="com",
            ),
        ),
    )
    
    await conversation.start()
    print("ğŸ™ï¸  ä¼šè©±ã‚’é–‹å§‹ã—ã¾ã—ãŸï¼")
    print("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: ã¯ã£ãã‚Šã¨è©±ã—ã¦ãã ã•ã„ã€‚è©±ã—çµ‚ã‚ã£ãŸã‚‰å°‘ã—å¾…ã¤ã¨èªè­˜ã•ã‚Œã¾ã™ã€‚")
    print("ğŸ›‘ çµ‚äº†: Ctrl+C")
    print("-" * 60)
    
    # Ctrl+Cã§çµ‚äº†
    signal.signal(signal.SIGINT, lambda _0, _1: asyncio.create_task(conversation.terminate()))
    
    # éŸ³å£°å…¥åŠ›ã‚’å‡¦ç†
    while conversation.is_active():
        chunk = await microphone_input.get_audio()
        conversation.receive_audio(chunk)


if __name__ == "__main__":
    print("\nğŸ”§ é«˜æ€§èƒ½éŸ³å£°èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­...")
    
    try:
        # ãƒã‚¤ã‚¯ãƒ†ã‚¹ãƒˆ
        r = sr.Recognizer()
        with sr.Microphone() as source:
            print("âœ… ãƒã‚¤ã‚¯: æº–å‚™å®Œäº†")
            r.adjust_for_ambient_noise(source, duration=1)
            print("âœ… ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«: èª¿æ•´å®Œäº†")
    except Exception as e:
        print(f"âŒ ãƒã‚¤ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        exit(1)
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ã”åˆ©ç”¨ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼")